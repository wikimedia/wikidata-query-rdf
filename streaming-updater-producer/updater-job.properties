##
# hostname for which the updater should capture the events for (filtering events based on meta.domain)
hostname: www.wikidata.org

##
# Job name for the updater pipeline
job_name: "WDQS Streaming Updater POC"

##
# custom routes to use to connect to various http services, default: none
# format is hostname1:https://target1:port,hostname2:https://target2:port
# http_routes: hostname1:https://target1:port,hostname2:https://target2:port

##
# http client timeout
# timeout for http connection: connectTime, readTime, soTime
# default: 5000 (millisec)
# http_timeout: 5000

##
# http user-agent
# defaults to "Wikidata Query Service Updater Bot"
# user_agent: Wikidata Query Service Updater Bot

##
# schema_base_uris
# defaults to https://schema.wikimedia.org/repositories/primary/jsonschema,https://schema.wikimedia.org/repositories/secondary/jsonschema
# schema_repositories: https://schema.wikimedia.org/repositories/primary/jsonschema,https://schema.wikimedia.org/repositories/secondary/jsonschema

##
# output_mutation_schema
# defaults to v1 (use v2 only once the consumers are able to read it)
# output_mutation_schema_version: v1

##
# namespaces of the entities the pipeline must capture events for
# 0: main, 120: properties, 146: lexemes
entity_namespaces: 0,120,146

##
# List of namespaces containing pages that may have a MCR slot managed by WikibaseMediaInfo that the pipeline must capture.
# Such namespace are suppose to produce page whose titles is written the editor and the EntityID is composed as 'M' + page_id
# In WMF setup only entity_namespaces is set when consuming wikidata and only mediainfo_entity_namespaces is set when consuming
# commons.wikimedia.org.
# 6: file
# mediainfo_entity_namespaces: 6

##
# path to the generated checkpoints
checkpoint_dir: hdfs://analytics-hadoop/wmf/discovery/streaming_updater/checkpoints

##
# Fine tune the kafka producer
# kafka_producer_config.$producer_config: $value
# Defaults to:
# kafka_producer_config.batch.size: 250000
# kafka_producer_config.linger.ms: 2000
# kafka_producer_config.compression.type: snappy
# Available settings are documented at:
# https://docs.confluent.io/platform/current/installation/configuration/producer-configs.html
# Note that not all properties can be tuned: *.serializer, bootstrap.servers,
# transaction.timeout.ms, delivery.timeout.ms and transactional.id must not be
# set here.

##
# Fine tune the kafka consumers
# kafka_consumer_config.$consumer_config: $value
# Empty by default.
# Example:
# kafka_consumer_config.security.protocol: SSL
# Available settings are documented at:
# https://docs.confluent.io/platform/current/installation/configuration/consumer-configs.html
# Note that not all properties can be tuned: *.serializer, bootstrap.servers,
# transaction.timeout.ms and delivery.timeout.ms not be set here.

##
# kafka brokers used to consume and produce messages
brokers: kafka-jumbo1007.eqiad.wmnet:9092

##
# Use newer event platform API to read/write to event streams
# Switch this flag is a breaking change and might require
# manually setting offsets to the corresponding input topics
# (default false)
# use_event_streams_api: false

####
# Legacy section used when use_event_streams_api is false
##
# prefixes for all the input topic names, to support the WMF multi-DC layout of the kafka topics
topic_prefixes: eqiad.,codfw.

##
# input topics
rev_create_topic: mediawiki.revision-create
page_delete_topic: mediawiki.page-delete
suppressed_delete_topic: mediawiki.page-suppress
page_undelete_topic: mediawiki.page-undelete

##
# optional reconciliation topic
# E.g. consume reconciliation events from rdf-streaming-updater.reconciliation filtered based on the
# "source_tag" reconciliation_source:
# reconciliation_topic: rdf-streaming-updater.reconciliation[source_tag]
##
# End of Legacy section used when use_event_streams_api is false
#####

#####
# Configuration section when use_event_streams_api is true
##
# name of the page_change stream
# no defaults
page_change_stream: mediawiki.page_change.v1

##
# Content models to filter from the main slot of page_change events (comma separated)
# in addition to namespace and hostname filter events can be filtered based on the content_model of
# their main slot
# no defaults
page_change_content_models=wikibase-item,wikibase-property,wikibase-lexeme

##
# optional reconciliation stream
# E.g. consume reconciliation events from rdf-streaming-updater.reconciliation filtered based on the
# "source_tag" reconciliation_source:
# reconciliation_stream: rdf-streaming-updater.reconciliation[source_tag]

##
# optional start timestamp to position kafka consumers when so offsets is recorded in the state
# no defaults (uses kafka managed consumer group offsets if present, latest if not)
# Accepts an iso8601 timestamp (defaults to latest if the timestamp is not found)
# kafka_topics_start_timestamp: 2024-01-01T00:00:00Z
##
# End of Configuration section when use_event_streams_api is true
#####

##
# kafka consumer group, used to fetch the initial set of offset then only used for monitoring
consumer_group: wdqs_streaming_updater_changeme

##
# endpoint to fetch the configuration for WMF event streams
# only used for side outputs running through kafka
stream_config_uri: https://meta.wikimedia.org/w/api.php

##
# output topic prefix (must be the current datacenter)
# only used by side outputs for now
output_topic_prefix: datacenter-changeme.

##
# domain to set in the meta subfield for side output events
# useful to distinguish these events when running multiple pipelines for the same
# wikibase instance
# defaults to the hostname property
# side_outputs_domain: custom.wikimedia.org

##
# kafka broker to use for side outputs
# optional, defaults to brokers
# side_outputs_kafka_brokers: kafka-jumbo1001.eqiad.wmnet:9092


##
# Identity of the emitter used to tag side output events
# default not set
# emitter_id: my_id

##
# Whether publish errors from side outputs to kafka or not
# Useful when testing when errors are expected and when no reconciliation batch jobs is set up
# defaults: true
# produce_side_outputs: true

##
# window length (ms) for the reordering operation
# reordering_window_length: 60000

##
# enforce general parallelism (excluding output operations that have to go with the parallelism 1)
# defaults: 1
parallelism: 1

##
# mediawiki_max_concurrent_requests
# The maximum number of concurrent requests made to mediawiki
# Cannot be set to a value less than parallelism and ideally must be set to a multiple of parallelism
# Defaults to 10
# Assuming we want to process 50evt/sec (~2x the realtime speed for wikidata) and the avg response time is 100ms and
# that we make 2 requests per events (diffs)  this is 50*2 = 100rps, 100*0.1 = 10 concurrent requests
# This value can be increased to reduce long backfills but have to be tuned carefully to not overload the target MW
# endpoint.
# mediawiki_max_concurrent_requests: 10

##
# Acceptable lag (in seconds) can have when fetching the RDF content (defaults to 10).
# The pipeline will keep retrying to fetch the RDF content if it receives HTTP 404
# and the event ingestion time is younger than this value in second.
# When the event becomes older it stops retrying and emit a fetch-failure side-output
# event.
# acceptable_mediawiki_lag: 10

##
# timeout for the generate diff async IO (defaults to 5minute) -1 to disable
generate_diff_timeout: -1

##
# Fine-tuning of the thread pool size for the generate diff async IO
# Defaults: calculated with (int) (mediawiki_max_concurrent_requests/parallelism)
# wikibase_repo_thread_pool_size: 10

##
# path to fetch the entity data (defaults to /wiki/Special:EntityData/)
# wikibase_entitydata_path: /wiki/Special:EntityData/

##
# output topic
output_topic: wdqs_streaming_updater_changeme

##
# name of the main output stream as defined in event platform's stream definitions
# required when using use_event_streams_api, defaults to rdf-streaming-updater.mutation otherwise.
# This is the "unversioned" name, v2 should be appended to it if use_event_streams_api is true.
main_output_stream: rdf-streaming-updater.mutation

##
# subgraph definition to use
# subgraph_definitions: wdqs-subgraph-definitions-v2
##
# output topic for subgraphs
# subgraph_kafka_topics.rdf-streaming-updater.mutation-main: wdqs_streaming_updater_main_changeme
# subgraph_kafka_topics.rdf-streaming-updater.mutation-scholarly: wdqs_streaming_updater_main_changeme

##
# output topic partition
# It is also used for subgraph_kafka_topics
output_topic_partition: 0

##
# idleness (ms) of input sources, default to 1minute
#
# - a value that is too low (< 1s) may incorrectly detect the stream as idle
# and might jump to higher watermarks issued from low rate streams and will end
# up marking as late events most events of this stream
#
# - a value that is too high (>?) may cause backfills to fail because too many
# events may be buffered in the window operation while idleness is reached.
# Later these events will likely be triggered at the same time causing too much
# backpressure and will cause the checkpoint to fail.
# (bug? some thread being blocked on
#  org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestMemorySegmentBlocking)
input_idleness: 2000

##
# max lateness (ms): allowed out-of-orderness
# max_lateness: 60000

##
# checkpoint_interval (ms)
checkpoint_interval: 30000

##
# checkpoint timeouts
checkpoint_timeout: 900000

##
# enable unaligned checkpoints, defaults to false
# unaligned_checkpoints: false

##
# min pause (ms) between checkpoints, defaults to 2seconds
# min_pause_between_checkpoints: 2000

##
# watermark intervals (ms), defaults to 200millis
# auto_wm_interval: 200

##
# enable/disable exactly_once, defaults to true
# exactly_once: true

##
# enable flink latency tracking (defaults disabled), in millisec
# latency_tracking_interval: 1000

##
# tune network buffer timeout (ms)
# network_buffer_timeout: 100

##
# configure the failure rate restart strategy
# https://ci.apache.org/projects/flink/flink-docs-stable/dev/task_failure_recovery.html#failure-rate-restart-strategy
# 2 failures max per 30min (allows failing once for the time of 2 * checkpoint timeouts)
restart_failures_rate_interval: 1800000
restart_failures_rate_max_per_interval: 2
restart_failures_rate_delay: 10000

##
# Uri scheme to use, either commons or wikidata
uris_scheme: wikidata

##
# Concepts URI for wikidata
# defaults:
#  - http:// + hostname when using uris_scheme: wikidata
#  - http://www.wikidata.org when using uris_scheme: commons
#
# wikidata_concept_uri: http://test.wikidata.org

##
# Concept URI for commons (defaults to https:// + hostname)
# only used when uris_scheme: commons
# commons_concept_uri: https://commons.wikimedia.org

##
# Quirks and workarounds
# ignore failures after transaction timeout: see https://issues.apache.org/jira/browse/FLINK-16419
# happens when trying to restore from an old savepoints (> 1week) with exactly_once semantic enabled
# defaults to false, should only be enabled to properly restore from such savepoints but should never be
# enabled in normal conditions.
# ignore_failures_after_transaction_timeout: false
